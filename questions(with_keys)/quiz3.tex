\documentclass[a4paper,10pt]{exam}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}
\usepackage{fancyhdr}
\lhead{}
\chead{Homework of Meachine Learning Techniques: Quiz3}
\rhead{}
\title{QUIZ3}
\date{}
\author{}
%\printanswers
\begin{document}
	\pagestyle{fancy}
	\maketitle
	\begin{questions}
		\question \textbf{Decision Tree}\\Impurity functions play an important role in decision tree branching. For binary classification problems, let $\mu_+$ be the fraction of positive examples in a data subset, and $\mu_- = 1 - \mu_+$ be the fraction of negative examples in the data subset.
		The Gini index is $1 - \mu_+^2 - \mu_-^2$. What is the maximum value of the Gini index among all $\mu_+ \in [0, 1]$?
		\begin{choices}
		\CorrectChoice 0.5
		\choice 0.75
		\choice 0.25
		\choice 0
		\choice 1\\
		\end{choices}
		
		\question Following Question 1, there are four possible impurity functions below. We can normalize each impurity function by dividing it with its maximum value among all $\mu_+ \in [0, 1]$ For instance, the classification error is simply $\min(\mu_+, \mu_-)$ and its maximum value is 0.5. So the normalized classification error is $2 \min(\mu_+, \mu_-)$. After normalization, which of the following impurity function is equivalent to the normalized Gini index?
		
		\begin{choices}
		  \CorrectChoice the squared regression error (used for branching in classification data sets), which is by definition $\mu_+ (1 - (\mu_+ - \mu_-))^2 + \mu_- (-1 - (\mu_+ - \mu_-))^2$.
		  \choice the entropy, which is $-\mu_+ \ln \mu_+ - \mu_- \ln \mu_-$, with $0 \log 0 \equiv 0$.
		  \choice the closeness, which is $1 - |\mu_+ - \mu_-|$.
		  \choice the classification error $\min(\mu_+, \mu_-)$.
		  \choice none of the other choices\\
		\end{choices}
		
		\question \textbf{Random Forest}\\
		If bootstrapping is used to sample $N' = pN$ examples out of $N$ examples and $N$ is very large. Approximately how many of the $N$ examples will not be sampled at all?
		\begin{choices}
			\choice $(1 - e^{-1/p}) \cdot N$
			\choice $(1 - e^{-p}) \cdot N$
			\choice $e^{-1} \cdot N$
			\choice $e^{-1/p} \cdot N$
			\CorrectChoice $e^{-p} \cdot N$\\
		\end{choices}
		
		\question Consider a Random Forest $G$ that consists of three binary classification trees $\{g_k\}_{k=1}^3$, where each tree is of test 0/1 error $E_{\text{out}}(g_1) = 0.1$, $E_{\text{out}}(g_2) = 0.2$, $E_{\text{out}}(g_3) = 0.3$. Which of the following is the exact possible range of $E_{\text{out}}(G)$?
		\begin{choices}
			\choice $0 \le E_{\text{out}}(G) \le 0.1$
			\choice $0.1 \le E_{\text{out}}(G) \le 0.6$
			\choice $0.2 \le E_{\text{out}}(G) \le 0.3$
			\choice $0.1 \le E_{\text{out}}(G) \le 0.3$
			\CorrectChoice $0.1 \le E_{\text{out}}(G) \le 0.3$\\
		\end{choices}
		
		\question Consider a Random Forest $G$ that consists of $K$ binary classification trees $\{g_k\}_{k=1}^K$, where $K$ is an odd integer. Each $g_k$ is of test 0/1 error $E_{\text{out}}(g_k) = e_k$. Which of the following is an upper bound of $E_{\text{out}}(G)$?
		\begin{choices}
			\CorrectChoice $\frac{2}{K+1} \sum_{k=1}^K e_k$
			\choice $\frac{1}{K} \sum_{k=1}^K e_k$
			\choice $\frac{1}{K+1} \sum_{k=1}^K e_k$
			\choice $\min_{1 \le k \le K} e_k$
			\choice $\max_{1 \le k \le K} e_k$\\
		\end{choices}
		
		\question \textbf{Gradient Boosting}\\
		Let $\epsilon_t$ be the weighted 0/1 error of each $g_t$ as described in the AdaBoost algorithm (Lecture 208), and $U_t = \sum_{n=1}^N u_n^{(t)}$ be the total example weight during AdaBoost. Which of the following equation expresses $U_{T+1}$ by $\epsilon_t$?
		\begin{choices}
			\choice none of the other choices
			\choice $\prod_{t=1}^T \epsilon_t$
			\choice $\sum_{t=1}^T (2 \sqrt{\epsilon_t(1-\epsilon_t)})$
			\choice $\sum_{t=1}^T \epsilon_t$
			\CorrectChoice $\prod_{t=1}^T (2 \sqrt{\epsilon_t(1-\epsilon_t)})$\\
		\end{choices}
		
		\question For the gradient boosted decision tree, if a tree with only one constant node is returned as $g_1$, and if $g_1(\mathbf{x}) = 2$, then after the first iteration, all $s_n$ is updated from $0$ to a new constant $\alpha_1 g_1(\mathbf{x}_n)$. What is $s_n$?
		
		\begin{choices}
			\choice 2
			\choice none of the other choices
			\choice $\max_{1 \le n \le N} y_n$
			\choice $\min_{1 \le n \le N} y_n$
			\CorrectChoice $\frac{1}{N} \sum_{n=1}^N y_n$\\
		\end{choices}
		
		\question For the gradient boosted decision tree, after updating all $s_n$ in iteration $t$ using the steepest $\eta$ as $\alpha_t$, what is the value of $\sum_{n=1}^N s_n g_t(\mathbf{x}_n)$?
		
		\begin{choices}
			\choice none of the other choices
			\CorrectChoice $\sum_{n=1}^N y_n g_t(\mathbf{x}_n)$
			\choice $\sum_{n=1}^N y_n^2$
			\choice $\sum_{n=1}^N y_n s_n$
			\choice 0	\\
	    \end{choices}
	    
	    \question \textbf{Neural Network}\\
	    Consider Neural Network with $\mbox{sign}(s)$ instead of $\tanh(s)$ as the transformation functions. That is, consider Multi-Layer Perceptrons. In addition, we will take $+1$ to mean logic TRUE, and ${{ - }}1$ to mean logic FALSE. Assume that all $x_i$ below are either $+1$ or ${{ - }}1$. Which of the following perceptron
	    \[g_A(\mathbf{x}) = \mbox{sign}\left(\sum_{i=0}^d w_i x_i\right).\]
	    implements
	    \[\text{OR}\left(x_1,x_2,\ldots,x_d\right).\]
	    \begin{choices}
	    	\CorrectChoice $(w_0, w_1, w_2, \cdots, w_d) = (d-1, +1, +1, \cdots, +1)$
	    	\choice $(w_0, w_1, w_2, \cdots, w_d) = (-d+1, -1, -1, \cdots, -1)$
	    	\choice none of the other choices
	    	\choice $(w_0, w_1, w_2, \cdots, w_d) = (d-1, -1, -1, \cdots, -1)$
	    	\choice $(w_0, w_1, w_2, \cdots, w_d) = (-d+1, +1, +1, \cdots, +1)$\\
	    \end{choices}
	    
	    \question Continuing from Question 9, among the following choices of $D$, which $D$ is the smallest for some $5$-$D$-$1$ Neural Network to implement $\text{XOR}\bigl(x_1,x_2,x_3,x_4, x_5\bigr)$?
	    \begin{choices}
	    	\choice 1
	    	\choice 9
	    	\choice 7
	    	\CorrectChoice 5
	    	\choice 3\\
	    \end{choices}
	    
	    \question For a Neural Network with at least one hidden layer and $\tanh(s)$ as the transformation functions on all neurons (including the output neuron), what is true about the gradient components (with respect to the weights) when all the initial weights $w_{ij}^{(\ell)}$ are set to $0$?
	    \begin{choices}
	    	\choice all the gradient components are zero
	    	\choice only the gradient components with respect to $w_{0j}^{(\ell)}$ for $j > 0$ may non-zero, all other gradient components must be zero
	    	\choice none of the other choices
	    	\choice only the gradient components with respect to $w_{j1}^{(L)}$ for $j>0$ may be non-zero, all other gradient components must be zero
	    	\CorrectChoice only the gradient components with respect to $w_{01}^{(L)}$ may be non-zero, all other gradient components must be zero \\
	    \end{choices}
	    
	    \question For a Neural Network with one hidden layer and $\tanh(s)$ as the transformation functions on all neurons (including the output neuron), what is always true about the backprop algorithm when all the initial weights $w_{ij}^{(\ell)}$ are set to $1$?
	    \begin{choices}
	    	\choice none of the other choices
	    	\CorrectChoice $w_{ij}^{(1)} = w_{i(j+1)}^{(1)}$ for all $i$ and $1 \le j{\rm{  < }}{d^{(1)}} - 1$
	    	\choice all $w_{j1}^{(2)}$ for $j>0$ are different
	    	\choice $w_{ij}^{(1)} = w_{(i+1)j}^{(1)}$ for $1 \le i{\rm{  < }}{d^{(0)}} - 1$ and all $j$
	    	\choice the gradient components with respect to all $w_{ij}^{(\ell)}$ are zero\\
	    \end{choices}
	    
	    \question \textbf{Experiments with Decision Tree}\\
	    Implement the simple C\&RT algorithm without pruning using the Gini index as the impurity measure as introduced in the class. For the decision stump used in branching, if you are branching with feature $i$ and direction $s$, please sort all the $x_{n, i}$ values to form (at most) $N+1$ segments of equivalent $\theta$, and then pick $\theta$ within the median of the segment. Run the algorithm on the following set for training:\\
	    \href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw3_data/hw3_train.dat}{hw3\_train.dat}\\
	    and the following set for testing:\\
	    \href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw3_data/hw3_test.dat}{hw3\_test.dat}\\
	    How many internal nodes (branching functions) are there in the resulting tree $G$?
	    \begin{choices}
	    	\choice 12
	    	\choice 8
	    	\choice 14
	    	\CorrectChoice 10
	    	\choice 6\\
	    \end{choices}
	    
	    \question Continuing from Question 13, which of the following is closest to the $E_{\text{in}}$ (evaluated with 0/1 error) of the tree?
	    \begin{choices}
	    	\CorrectChoice 0.0
	    	\choice 0.1
	    	\choice 0.2
	    	\choice 0.3
	    	\choice 0.4\\
	    \end{choices}
	    
	    \question Continuing from Question 13, which of the following is closest to the $E_{\text{out}}$ (evaluated with 0/1 error) of the tree?
	    \begin{choices}
	    \choice 0.05
	    \choice 0.25
	    \choice 0.35
	    \choice 0.00
	    \CorrectChoice 0.15\\
	    \end{choices}
	    
	    \question Now implement the Bagging algorithm with $N' = N$ and couple it with your decision tree above to make a preliminary random forest $G_{RS}$. Produce $T=300$ trees with bagging. Repeat the experiment for 100 times and compute average $E_{\text{in}}$ and $E_{\text{out}}$ using the 0/1 error.
	    Which of the following is true about the average $E_{\text{in}}(g_t)$ for all the 30000 trees that you have generated?
	    \begin{choices}
	    	\CorrectChoice $0.03 \leq \mbox{average } E_{\text{in}}(g_t) <0.06$
	    	\choice $0.00 \leq \mbox{average } E_{\text{in}}(g_t) < 0.03$
	    	\choice $0.09 \leq \mbox{average } E_{\text{in}}(g_t) < 0.12$
	    	\choice $0.06 \leq \mbox{average } E_{\text{in}}(g_t) < 0.09$
	    	\choice $0.12 \leq \mbox{average } E_{\text{in}}(g_t) < 0.50$\\
	    \end{choices}
	    
	    \question Continuing from Question 16, which of the following is true about the average $E_{\text{in}}(G_{RF})$?
	    \begin{choices}
	    	\choice $0.06 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.09$
	    	\choice $0.09 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.12$
	    	\choice $0.12 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.50$
	    	\CorrectChoice $0.12 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.50$
	    	\choice $0.03 \leq \mbox{average } E_{\text{in}}(G_{RF}) < 0.06$\\
	    \end{choices}
	    
	    \question Continuing from Question 16, which of the following is true about the average $E_{\text{out}}(G_{RF})$?
	    \begin{choices}
	    	\CorrectChoice $0.06 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.09$
	    	\choice $0.09 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.12$
	    	\choice $0.03 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.06$
	    	\choice $0.00 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.03$
	    	\choice $0.12 \leq \mbox{average } E_{\text{out}}(G_{RF}) < 0.50$\\
	    \end{choices}
	    
	    \question Now, `prune' your decision tree algorithm by restricting it to have one branch only. That is, the tree is simply a decision stump determined by Gini index. Make a random `forest' $G_{RS}$ with those decision stumps with Bagging like Questions 16-18 with $T=300$. Repeat the experiment for 100 times and compute average $E_{\text{in}}$ and $E_{\text{out}}$ using the 0/1 error.
	    Which of the following is true about the average $E_{\text{in}}(G_{RS})$?
	    
	    \begin{choices}
	    	\CorrectChoice $0.09 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.12$
	    	\choice $0.03 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.06$
	    	\choice $0.00 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.03$
	    	\choice $0.12 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.50$
	    	\choice $0.06 \leq \mbox{average } E_{\text{in}}(G_{RS}) < 0.09$\\
	    \end{choices}
	    
	    \question Continuing from Question 19, which of the following is true about the average $E_{\text{out}}(G_{RS})$?
	    \begin{choices}
	      \choice $0.06 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.09$
	      \choice $0.09 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.12$
	      \choice $0.03 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.06$
	      \choice $0.00 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.03$
	      \CorrectChoice $0.12 \leq \mbox{average } E_{\text{out}}(G_{RS}) < 0.50$\\
	   \end{choices}
	\end{questions}
\end{document}
