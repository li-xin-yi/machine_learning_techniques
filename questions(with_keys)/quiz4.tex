\documentclass[a4paper,10pt]{exam}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage[colorlinks,linkcolor=red,anchorcolor=blue,citecolor=green]{hyperref}
\usepackage{fancyhdr}
\lhead{}
\chead{Homework of Meachine Learning Techniques: Quiz4(with keys)}
\rhead{}
\title{QUIZ4}
\date{}
\author{}
\printanswers
\begin{document}
	\pagestyle{fancy}
	\maketitle
	\begin{questions}
		\question \textbf{Neural Network and Deep Learning}\\
		A fully connected Neural Network has $L=2; \, d^{(0)}=5, \, d^{(1)}=3, \, d^{(2)}=1$. If only products of the form $w_{ij}^{(\ell)} x_i^{(\ell-1)}$, $w_{ij}^{(\ell+1)} \delta_j^{(\ell+1)}$, and $x_i^{(\ell-1)} \delta_j^{(\ell)}$ count as operations (even for $x_0^{(\ell-1)}=1$), without counting anything else, which of the following is the total number of operations required in a single iteration of backpropagation (using SGD on one data point)?
		\begin{choices}
			\CorrectChoice 47
			\choice 43
			\choice 53
			\choice 59
			\choice none of the other choices\\
		\end{choices}
		
		\question Consider a Neural Network without any bias terms $x_{0}^{(\ell)}$. Assume that the network contains $d^{(0)}=10$ input units, 1 output unit, and 36 hidden units. The hidden units can be arranged in any number of layers $\ell=1,\cdots,L-1$, and each layer is fully connected to the layer above it. What is the minimum possible number of weights that such a network can have?
		\begin{choices}
			\CorrectChoice 46
			\choice 44
			\choice none of the other choices
			\choice 43
			\choice 45\\
		\end{choices}
		
		\question Following Question 2, what is the maximum possible number of weights that such a network can have?
		\begin{choices}
			\choice 510
			\choice 520
			\CorrectChoice none of the other choices
			\choice 500
			\choice 490\\
		\end{choices}
		
		\question \textbf{Autoencoder}\\
		Assume an autoencoder with $\tilde{d} = 1$. That is, the $d \times \tilde{d}$ weight matrix $W$ becomes a $d \times 1$ weight vector $\mathbf{w}$, and the linear autoencoder tries to minimize 
		\[E_{in}(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^N \|\mathbf{x}_n - \mathbf{w} \mathbf{w}^T \mathbf{x}_n\|^2.\]
		We can solve this problem with stochastic gradient descent by defining
		\[\text{err}_n(\mathbf{w}) = \|\mathbf{x}_n - \mathbf{w} \mathbf{w}^T \mathbf{x}_n\|^2\]
		and calculate $\nabla_\mathbf{w} \text{err}_n(\mathbf{w})$. What is $\nabla_\mathbf{w} \text{err}_n(\mathbf{w})$?
		\begin{choices}
			\choice $(4 \mathbf{x}_n - 4) (\mathbf{w}^T \mathbf{w})$
			\choice none of the other choices
			\choice $(4 \mathbf{w} - 4) (\mathbf{x}_n^T \mathbf{x}_n)$
			\CorrectChoice $2 (\mathbf{x}_n^T \mathbf{w})^2 \mathbf{w} + 2 (\mathbf{x}_n^T \mathbf{w}) (\mathbf{w}^T \mathbf{w})$
			\choice $2 (\mathbf{x}_n^T \mathbf{w})^2 \mathbf{x}_n + 2 (\mathbf{x}_n^T \mathbf{w}) (\mathbf{w}^T \mathbf{w}) \mathbf{w} - 4 (\mathbf{x}_n^T \mathbf{w}) \mathbf{w}$\\
		\end{choices}
		
		\question Following Question 4, assume that noise vectors ${\boldsymbol\epsilon}_n$ are generated i.i.d. from a zero-mean, unit var iance Gaussian distribution and added to $\mathbf{x}_n$ to make $\tilde{\mathbf{x}}_n = \mathbf{x}_n + {\boldsymbol\epsilon}_n$, a noisy version of $\mathbf{x}_n$. Then, the linear denoising autoencoder tries to minimize
		\[E_{in}(\mathbf{w}) = \frac{1}{N} \sum_{n=1}^N \|\mathbf{x}_n - \mathbf{w} \mathbf{w}^T (\mathbf{x}_n + {\boldsymbol\epsilon}_n)\| ^2\]
		For any fixed $\mathbf{w}$, what is $\mathcal{E}\left(E_{in}(\mathbf{w})\right)$, where the expectation $\mathcal{E}$ is taken over the noise generation process?
		\begin{choices}
			\choice $\frac{1}{N} \sum_{n=1}^N \|\mathbf{x}_n - \mathbf{w} \mathbf{w}^T \mathbf{x}_n\|^2$
			\CorrectChoice $\frac{1}{N} \sum_{n=1}^N \|\mathbf{x}_n - \mathbf{w} \mathbf{w}^T \mathbf{x}_n\|^2 + d \mathbf{w}^T \mathbf{w}$
			\choice none of the other choices
			\choice $\frac{1}{N} \sum_{n=1}^N \|\mathbf{x}_n - \mathbf{w} \mathbf{w}^T \mathbf{x}_n\|^2 + \mathbf{w}^T \mathbf{w}$
			\choice $\frac{1}{N} \sum_{n=1}^N \|\mathbf{x}_n - \mathbf{w} \mathbf{w}^T \mathbf{x}_n\|^2 + \frac{1}{d} \mathbf{w}^T \mathbf{w}$\\
		\end{choices}
		
		\question \textbf{Nearest Neighbor and RBF Network}\\
		Consider getting the 1 Nearest Neighbor hypothesis from a data set of two examples $(\mathbf{x}_+, +1)$ and $(\mathbf{x}_-, -1)$. Which of the following linear hypothesis $g_{LIN}(\mathbf{x}) = \mbox{sign}(\mathbf{w}^T \mathbf{x} + b)$ (where $\mathbf{w}$ does not include $b = w_0$) is equivalent to the hypothesis?
		\begin{choices}
			\choice none of the other choices
			\choice $\mathbf{w} = 2 (\mathbf{x}_+ - \mathbf{x}_-)$, $b = + \mathbf{x}_+^T \mathbf{x}_-$
			\choice $\mathbf{w} = 2 (\mathbf{x}_- - \mathbf{x}_+)$, $b = + \|\mathbf{x}_+\|^2 - \|\mathbf{x}_-\|^2$
			\choice $\mathbf{w} = 2 (\mathbf{x}_- - \mathbf{x}_+)$, $b = -\mathbf{x}_+^T \mathbf{x}_-$
			\CorrectChoice $\mathbf{w} = 2 (\mathbf{x}_+ - \mathbf{x}_-)$, $b = - \|\mathbf{x}_+\|^2 + \|\mathbf{x}_-\|^2$\\
		\end{choices}
		
		\question Consider an RBF Network hypothesis for binary classification 
		\[g_{RBFNET}(\mathbf{x}) = \mbox{sign}\left(\beta_+ \exp(-\|\mathbf{x} - {\boldsymbol\mu}_+\|^2) + \beta_- \exp(-\|\mathbf{x} - {\boldsymbol\mu}_-\|^2)\right)\]
		and assume that $\beta_+ > 0 > \beta_-$ . Which of the following linear hypothesis $g_{LIN}(\mathbf{x}) = \mbox{sign}(\mathbf{w}^T \mathbf{x} + b)$ (where $\mathbf{w}$ does not include $b = w_0$) is equivalent to $g_{RBFNET}(\mathbf{x})$?
		\begin{choices}
			\CorrectChoice $\mathbf{w} = 2 ({\boldsymbol\mu}_+ - {\boldsymbol\mu}_-)$, $b = \ln \left|\frac{\beta_+}{\beta_-}\right| - \|{\boldsymbol\mu}_+\|^2 + \|{\boldsymbol\mu}_-\|^2$
			\choice $\mathbf{w} = 2 ({\boldsymbol\mu}_- - {\boldsymbol\mu}_+)$, $b = \ln \left|\frac{\beta_-}{\beta_+}\right| + \|{\boldsymbol\mu}_+\|^2 - \|{\boldsymbol\mu}_-\|^2$
			\choice $\mathbf{w} = 2 (\beta_+ {\boldsymbol\mu}_+ + \beta_- {\boldsymbol\mu}_-)$, $b = - \beta_+\|{\boldsymbol\mu}_+\|^2 + \beta_- \|{\boldsymbol\mu}_-\|^2$
			\choice $\mathbf{w} = 2 (\beta_+ {\boldsymbol\mu}_+ + \beta_- {\boldsymbol\mu}_-)$, $b = + \beta_+\|{\boldsymbol\mu}_+\|^2 - \beta_- \|{\boldsymbol\mu}_-\|^2$
			\choice none of the other choices\\
	   \end{choices}
	   
	   \question Assume that a full RBF network (page 9 of class 214) using $\mbox{RBF}(\mathbf{x}, {\boldsymbol\mu}) = [[\mathbf{x} = {    \boldsymbol\mu}]]$ is solved for squared error regression on a data set where all inputs $\mathbf{x}_n$ are different. What are the optimal coefficients $\beta_n$ for each $\mbox{RBF}(\mathbf{x}, \mathbf{x}_n)$?
	   \begin{choices}
	   	 \CorrectChoice $y_n$
	   	 \choice $\|\mathbf{x}_n\|^2 y_n^2$
	   	 \choice none of the other choices
	   	 \choice $\|\mathbf{x}_n\| y_n$
	   	 \choice $y_n^2$\\
	   \end{choices}
		
	   \question \textbf{Matrix Factorization}\\
	   Consider matrix factorization of $\tilde{d} = 1$ with alternating least squares. Assume that the $\tilde{d} \times N$ user factor matrix $\mathrm{V}$ is initialized to a constant matrix of 1. After step 2.1 of alternating least squares (page 10 of lecture 215), what is the optimal $w_m$, the $\tilde{d} \times 1$ movie `vector' for the m-th movie?
	   \begin{choices}
	   	\CorrectChoice the average rating of the $m$-th movie
	   	\choice the total rating of the $m$-th movie
	   	\choice the maximum rating of the $m$-th movie
	   	\choice the minimum rating of the $m$-th movie
	   	\choice none of the other choices\\
	   \end{choices}
	   
	   \question Assume that for a full rating matrix $\mathrm{R}$, we have obtained a perfect matrix factorization $\mathrm{R} = \mathrm{V}^T \mathrm{W}$. That is, $r_{nm} = \mathbf{v}_n^T \mathbf{w}_m$ for all $n$,$m$. Then, a new user $(N+1)$ comes. Because we do not have any information for the type of the movie she likes, we initialize her feature vector $\mathbf{v}_{N+1}$ to $\frac{1}{N} \sum_{n    =1}^N \mathbf{v}_n$, the average user feature vector. Now, our system decides to recommend her a movie $m$ with the maximum predicted score $\mathbf{v}_{N+1}^T \mathbf{w}_m$. What would the movie be?
	   \begin{choices}
	   	\choice the movie with the largest maximum rating
	   	\choice none of the other choices
	   	\choice the movie with the smallest rating variance
	   	\choice the movie with the largest minimum rating
	   	\CorrectChoice the movie with the largest average rating\\
	   \end{choices}
	   
	   \question \textbf{Experiment with Backprop neural Network }\\
	   \\Implement the backpropagation algorithm (page 16 of lecture 212) for $d$-$M$-$1$ neural network with $\tanh$-type neurons, \textbf{including the output neuron}. Use the squared error measure between the output $g_{NNET}(\mathbf{x}_n)$ and the desired $y_n$ and backprop to calculate the per-example gradient. Because of the different output neuron, your $\delta_1^{(L)}$ would be different from the course slides! Run the algorithm on the following set for training (each row represents a pair of $(\mathbf{x}_n, y_n)$; the first column is $(\mathbf{x}_n)_1$; the second one is $(\mathbf{x}_n)_2$; the third one is $y_n$):\\
	   \href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw4_data/hw4_nnet_train.dat}{hw4\_nnet\_train.dat}\\
	   and the following set for testing:\\
	   \href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw4_data/hw4_nnet_test.dat}{hw4\_nnet\_test.dat}\\
	   \\Fix $T=50000$ and consider the combinations of the following parameters:
	   \begin{itemize}
	   	\item the number of hidden neurons $M$
	   	\item the elements of $w_{ij}^{(\ell)}$ chosen independently and uniformly from the range $(-r, r)$
	   	\item the learning rate $\eta$
	   \end{itemize}
	   Fix $\eta = 0.1$ and $r = 0.1$. Then, consider $M \in \{1, 6, 11, 16, 21\}$ and repeat the experiment for 500 times. Which $M$ results in the lowest average $E_{out}$ over 500 experiments?
	   \begin{choices}
	   	 \choice 11
	   	 \choice 16
	   	 \choice 1
	   	 \choice 21
	   	 \CorrectChoice 6\\
	   \end{choices}
	   
	   \question Following Question 11, fix $\eta = 0.1$ and $M=3$. Then, consider $r \in \{0, 0.001, 0.1, 10, 1000\}$ and repeat the experiment for 500 times. Which $r$ results in the lowest average $E_{out}$ over 500 experiments?
	   \begin{choices}
	   	\choice 0
	   	\CorrectChoice 0.1
	   	\choice 0.001
	   	\choice 10
	   	\choice 1000\\
	   \end{choices}
	   
	   \question Following Question 11, fix $r=0.1$ and $M=3$. Then, consider $\eta \in \{0.001, 0.01, 0.1, 1, 10\}$ and repeat the experiment for 500 times. Which $\eta$ results in the lowest average $E_{out}$ over 500 experiments?
	   \begin{choices}
	   	\CorrectChoice 0.01
	   	\choice 0.001
	   	\choice 10
	   	\choice 0.1
	   	\choice 1\\
	   \end{choices}
	   
	   \question Following Question 11, deepen your algorithm by making it capable of training a $d$-$8$-$3$-$1$ neural network with $\tanh$-type neurons. Do not use any pre-training. Let $r=0.1$ and $\eta = 0.01$ and repeat the experiment for 500 times. Which of the following is true about $E_{out}$ over 500 experiments?
	   \begin{choices}
	   	\CorrectChoice $0.02 \leq E_{out} < 0.04$
	   	\choice none of the other choices
	   	\choice $0.04 \leq E_{out} < 0.06$
	   	\choice $0.06 \leq E_{out} < 0.08$
	   	\choice $0.00 \leq E_{out} < 0.02$\\ 
	   \end{choices}
	   
	   \question \textbf{Experiment with 1 Nearest Neighbor}\\
	   Implement any algorithm that `returns' the $1$ Nearest Neighbor hypothesis discussed in page 8 of lecture 214. 
	   \[g_{\text{nbor}}(\mathbf{x}) = y_m \mbox{ such that } \mathbf{x} \mbox{ closest to } \mathbf{x}_m\]
	   Run the algorithm on the following set for training:\\
	   \href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw4_data/hw4_knn_train.dat}{hw4\_knn\_train.dat}\\
	   and the following set for testing:\\
	   \href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw4_data/hw4_knn_test.dat}{hw4\_knn\_test.dat}
	   \\Which of the following is closest to $E_{in}(g_{\text{nbor}})$?
	   \begin{choices}
	   	\choice 0.2
	   	\choice 0.3
	   	\CorrectChoice 0.0
	   	\choice 0.1
	   	\choice 0.4\\
	   \end{choices}
	   
	   \question Following Question 15, which of the following is closest to $E_{out}(g_{\text{nbor}})$?
	   \begin{choices}
	   	\choice 0.30
	   	\choice 0.28
	   	\CorrectChoice 0.34
	   	\choice 0.32
	   	\choice 0.26\\
	   \end{choices}
	   
	   \question Now, implement any algorithm for the $k$ Nearest Neighbor with $k=5$ to get $g_{5\text{-nbor}}(\mathbf{x})$. Run the algorithm on the same sets in Question 15 for training/testing.
	   Which of the following is closest to $E_{in}(g_{5\text{-nbor}})$?
	   \begin{choices}
	   	\choice 0.1
	   	\CorrectChoice 0.2
	   	\choice 0.3
	   	\choice 0.4
	   	\choice 0.0\\
	   \end{choices}
	   
	   \question Following Question 17, Which of the following is closest to $E_{out}(g_{5\text{-nbor}})$
	   \begin{choices}
	   	\choice 0.28
	   	\choice 0.26
	   	\choice 0.34
	   	\CorrectChoice 0.32
	   	\choice 0.30\\
	   \end{choices}
	   
	   \question \textbf{Experiment with k-Means}
	   Implement the $k$-Means algorithm (page 16 of lecture 214).Randomly select $k$ instances from $\{\mathbf{x}_n\}$ to initialize your ${\boldsymbol\mu}_m$ Run the algorithm on the following set for training:\\
	   \href{https://d396qusza40orc.cloudfront.net/ntumltwo/hw4_data/hw4_kmeans_train.dat}{hw4\_kmeans\_train.dat }\\
	   and repeat the experiment for 500 times. Calculate the clustering $E_{in}$ by $\frac{1}{N} \sum_{n=1}^N \sum_{m=1}^M [[\mathbf{x}_n \in S_m]] \|\mathbf{x}_n - {\boldsymbol\mu}_m\|^2$\\
	   as described on page 13 of lecture 214 for $M = k$.\\
	   For $k=2$, which of the following is closest to the average $E_{in}$ of $k$-Means over 500 experiments?
	   \begin{choices}
	   	\choice 0.5
	   	\choice 1.0
	   	\CorrectChoice 2.5
	   	\choice 1.5
	   	\choice 2.0\\
	   \end{choices}
	   
	   \question For $k=10$, which of the following is closest to the average $E_{in}$ of $k$-Means over 500 experiments?
	   \begin{choices}
	   	\choice 1.0
	   	\CorrectChoice 1.5
	   	\choice 2.0
	   	\choice 0.5
	   	\choice 2.5
	   \end{choices}
	\end{questions}
\end{document}